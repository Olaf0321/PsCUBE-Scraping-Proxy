import asyncio
from playwright.async_api import async_playwright
import random
import re
from datetime import datetime, timedelta
import os
import csv
import json
import sys
from google.oauth2 import service_account
from googleapiclient.discovery import build
import pandas as pd

IPLOG_CSV = "ip_log.csv"
IPLOG_XLSX = "ip_log.xlsx"
IP_CHECK_INTERVAL_SEC = 600

def init_csvs():
    if not os.path.exists(IPLOG_CSV):
        with open(IPLOG_CSV, "w", newline="", encoding="utf-8") as f:
            w = csv.writer(f)
            w.writerow(["timestamp","ip","country","note"])

def append_row(path, row):
    with open(path, "a", newline="", encoding="utf-8") as f:
        csv.writer(f).writerow(row)

async def ip_logger_task(context):
    """Log the current public IP + country every minute using the SAME proxy + session."""
    while True:
        page = await context.new_page()
        try:
            ip, country = "", ""

            # get IP
            await page.goto("http://api.ipify.org?format=json", timeout=30000)
            ip_json = await page.evaluate("() => document.body.innerText")
            try:
                ip = json.loads(ip_json).get("ip", "")
            except:
                ip = ip_json.strip()

            # get country
            await page.goto("http://ip-api.com/json", timeout=30000)
            meta_json = await page.evaluate("() => document.body.innerText")
            try:
                country = json.loads(meta_json).get("country", "")
            except:
                country = meta_json.strip()

            stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            append_row(IPLOG_CSV, [stamp, ip, country, "rotating-endpoint"])
            print(f"[IP] {stamp} | {ip} | {country}")

        except Exception as e:
            stamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            append_row(IPLOG_CSV, [stamp, "", "", f"ip-check-error: {e}"])
            print(f"[IP] error: {e}")

        finally:
            await page.close()

        await asyncio.sleep(IP_CHECK_INTERVAL_SEC)


def csv_to_xlsx(csv_path: str, xlsx_path: str):
    try:
        # lightweight conversion without extra deps
        df = pd.read_csv(csv_path, encoding="utf-8")
        df.to_excel(xlsx_path, index=False)
    except Exception as e:
        print(f"[xlsx] Could not write {xlsx_path}: {e}. (CSV is still saved.)")

def set_stdout(to_file=True):
    if to_file:
        sys.stderr = sys.stdout
    else:
        sys.stderr = sys.__stderr__

def get_checked_rows():
    SHOP_SPREADSHEET_ID = "1fWsztueWu0xxtcZn-FRPzxJaHV1MhbPwcUOi23rV9lY"
    RANGE_NAME = "A1:D"
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']
    SERVICE_ACCOUNT_FILE = "weighty-vertex-464012-u4-7cd9bab1166b.json"
    # èªè¨¼
    creds = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    
    service = build('sheets', 'v4', credentials=creds)
    sheet = service.spreadsheets()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    result = sheet.values().get(spreadsheetId=SHOP_SPREADSHEET_ID,
                                range=RANGE_NAME).execute()
    values = result.get('values', [])
    
    if not values:
        print('ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚')
        return []

    headers = values[0]
    checked_rows = []

    # ãƒ‡ãƒ¼ã‚¿è¡Œã‚’å‡¦ç†ï¼ˆ1è¡Œç›®ã¯ãƒ˜ãƒƒãƒ€ãƒ¼ï¼‰
    for row in values[1:]:
        # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹åˆ—ãŒ 'TRUE' ã®è¡Œã®ã¿å¯¾è±¡
        if len(row) > 0 and row[0].strip().upper() == 'TRUE':
            row_data = {headers[i]: row[i] if i < len(row) else '' for i in range(len(headers))}
            checked_rows.append(row_data)
    
    return checked_rows

def sanitize_filename(filename: str) -> str:
    # Windowsã§ä½¿ãˆãªã„æ–‡å­—ã‚’é™¤å»ã¾ãŸã¯ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã«å¤‰æ›
    return re.sub(r'[<>:"/\\|?*]', '_', filename)

async def human_like_scroll(page, scroll_offset):
    total_scrolled = 0

    while total_scrolled < scroll_offset:
        remaining = scroll_offset - total_scrolled

        # If less than 100px left, scroll that exact amount
        if remaining < 100:
            step = remaining
        else:
            step = random.randint(100, min(500, remaining))

        await page.evaluate(f"window.scrollBy(0, {step})")
        total_scrolled += step

        await asyncio.sleep(random.uniform(0.1, 0.4))

def append_row_to_csv(row_data, filename):
    with open(filename, mode='a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(row_data)

def extract_sheet_id_from_url(url: str) -> str:
    match = re.search(r'/d/([a-zA-Z0-9-_]+)', url)
    if match:
        return match.group(1)
    else:
        return None

def get_current_sheet_date(slot_sheet_url):
    SLOT_SPREADSHEET_ID = extract_sheet_id_from_url(slot_sheet_url)
    RANGE_NAME = "A2:A2"
    SCOPES = ['https://www.googleapis.com/auth/spreadsheets.readonly']
    SERVICE_ACCOUNT_FILE = "weighty-vertex-464012-u4-7cd9bab1166b.json"
    # èªè¨¼
    creds = service_account.Credentials.from_service_account_file(
        SERVICE_ACCOUNT_FILE, scopes=SCOPES)
    
    service = build('sheets', 'v4', credentials=creds)
    sheet = service.spreadsheets()
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—
    result = sheet.values().get(spreadsheetId=SLOT_SPREADSHEET_ID,
                                range=RANGE_NAME).execute()
    values = result.get('values', [])
    
    print(f"å–å¾—ã—ãŸã‚·ãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿: {values}")
    current_sheet_date = values[0][0] if values else None
    if current_sheet_date:
        print(f"ç¾åœ¨ã®ã‚·ãƒ¼ãƒˆã®æ—¥ä»˜: {current_sheet_date}")
    else:
        print("ã‚·ãƒ¼ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    return current_sheet_date

async def eachMachineFunc(page, model_name, scrap_days):
    result = []
    await asyncio.sleep(1)

    # await page.wait_for_load_state("load")
    title = await page.title()
    print(f"å–å¾—ã—ãŸæ–°ã—ã„ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
    await human_like_scroll(page, 500)
    # await asyncio.sleep(5)

    extracted_data = [
        "2025-06-28",           # æ—¥ä»˜
        title,                 # æ©Ÿç¨®åï¼ˆã“ã“ã§ã¯ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ©Ÿç¨®åã®ä»£ã‚ã‚Šã«ä½¿ç”¨ï¼‰
        "123",                 # å°ç•ªå·
        "1000",                # æŠ•å…¥æšæ•°
        "-200",                # å·®æšæ•°
        "5",                   # BIGå›æ•°
        "2",                   # REGå›æ•°
        "2",                   # AT/ARTå›æ•°
        "500",                 # ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ
        "150",                 # æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ
    ]
    
    # ç¾åœ¨ã®æ—¥ä»˜ã¨æ™‚åˆ»ã‚’å–å¾—
    now = datetime.now()

    machine_no = ''

    # div#divDAI ã®ä¸­ã® <h2> ã‚’å–å¾—
    h2_element = await page.query_selector('#divDAI h2')

    if h2_element:
        h2_text = await h2_element.inner_text()

        # æ­£è¦è¡¨ç¾ã§æœ€å¾Œã®4æ¡ã®æ•°å­—ã‚’æŠ½å‡º
        match = re.search(r'(\d{4})\s*$', h2_text)
        if match:
            machine_no = match.group(1)
        else:
            print("4æ¡ã®ç•ªå·ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        print("div#divDAI h2 ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

    # "scroll" ã‚¯ãƒ©ã‚¹ã® div å†…ã® <tr> ã‚’å–å¾—
    try:
        scroll_div = await page.wait_for_selector('div.scroll', timeout=3000)
        tr = await scroll_div.query_selector('tr')
    except Exception as e:
        print("âš ï¸ scroll_div ã¾ãŸã¯ tr ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ:", e)
        return


    # tr å†…ã®ã™ã¹ã¦ã® <td> ã‚’å–å¾—
    tds = await tr.query_selector_all('td')

    # format check
    # Select all the inner divs inside the td
    elements = await page.query_selector_all('td.nc-grid-color-fix.nc-text-align-center div.inner.nc-background-image-00')
    check_text = (await elements[3].inner_text()).strip()

    print(f"ãƒã‚§ãƒƒã‚¯ãƒ†ã‚­ã‚¹ãƒˆ: {check_text}")

    if check_text == "AT/ART":
        indices_to_extract = [1, 2, 3, 5, 6]
    else: 
        indices_to_extract = [1, 2, 6, 7]

    # æœ€åˆã® td ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆindex 0ï¼‰ã€2ç•ªç›®ä»¥é™ã«å¯¾ã—ã¦å‡¦ç†
    for i, td in enumerate(tds[1:1+scrap_days], start=1):
        if i > 6:
            break  # æœ€å¤§6æ—¥å‰ã¾ã§ã«åˆ¶é™
        # td å†…ã®ã™ã¹ã¦ã® <div class="outer border-bottom"> ã‚’å–å¾—
        divs = await td.query_selector_all('div.outer.border-bottom')

        values = []
        for j in indices_to_extract:
            if j < len(divs):
                inner_div = await divs[j].query_selector('div.inner.nc-text-align-right')
                if inner_div:
                    text = await inner_div.inner_text()
                    values.append(text.strip())
                else:
                    values.append("")  # inner div not found
            else:
                values.append("")  # index out of range
        
        target_date = now - timedelta(days=i)
        formatted_date = target_date.strftime("%Y/%m/%d")
        extracted_data[0] = formatted_date
        extracted_data[1] = model_name
        extracted_data[2] = machine_no
        extracted_data[3] = extracted_data[4] = ''
        extracted_data[5] = values[0]
        extracted_data[6] = values[1]
        extracted_data[7] = values[2] if len(values) == 5 else ''
        extracted_data[8] = values[3] if len(values) == 5 else values[2]
        extracted_data[9] = values[4] if len(values) == 5 else values[3]

        # ãƒ‡ãƒ¼ã‚¿ã‚’æ›¸ãè¾¼ã¿
        result.append(extracted_data.copy())
    return result

async def eachModelFunc(page, full_url, model_name, filename, scrap_days):
    max_retries = 10
    attempt = 0
    target_td = None

    await page.goto(full_url, timeout=60000)
    await page.wait_for_load_state("load")
    await asyncio.sleep(1)

    while attempt < max_retries:
        attempt += 1
        print(f"[INFO] Accessing {full_url}, attempt {attempt}/{max_retries}")
    
        try:
            await page.wait_for_selector(
                "td.nc-grid-color-fix.nc-text-align-center", 
                timeout=15000
            )
        except Exception:
            print("[WARN] Target element not found, retrying...")
            await page.reload(timeout=60000)
            # await page.go_back(timeout=60000)
            # await asyncio.sleep(1)
            # await page.goto(full_url, timeout=60000)
            await asyncio.sleep(1)
            continue  # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ãƒªãƒˆãƒ©ã‚¤

            # è¦ç´ å–å¾—
        target_td = await page.query_selector('td.nc-grid-color-fix.nc-text-align-center')
        break

    if not target_td:
        raise Exception(f"Target <td> not found after {max_retries} retries.")

    # Step 2: Find all <div class="outer border-bottom"> inside that <td>
    divs = await target_td.query_selector_all('div.outer.border-bottom')

    # Step 3: Store the index positions instead of element handles
    num_divs = len(divs)

    response_data = None

    async def handle_response(response):
        nonlocal response_data
        if "nc-m06-001.php" in response.url and response.status == 200:
            try:
                json_data = await response.json()
                # print("âœ… ãƒ¬ã‚¹ãƒãƒ³ã‚¹å–å¾—æˆåŠŸ:", response.url)
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                with open("slump_graph.json", "w", encoding="utf-8") as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                response_data = json_data
            except Exception as e:
                print("âŒ JSONè§£æå¤±æ•—:", e)

    page.on("response", handle_response)

    # Step 4: Loop by index and re-fetch the element each time
    for i in range(1, num_divs):
        await asyncio.sleep(1)
        # Re-query target_td and divs each time after navigation
        target_td = await page.query_selector('td.nc-grid-color-fix.nc-text-align-center')
        if not target_td:
            raise Exception("Target <td> not found after navigation.")

        divs = await target_td.query_selector_all('div.outer.border-bottom')
        if i >= len(divs):
            print(f"ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ {i} ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚ãƒšãƒ¼ã‚¸å†…ã® div è¦ç´ ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚")
            continue

        div = divs[i]

        await human_like_scroll(page, 300)
        await div.click(timeout=60000)
        await asyncio.sleep(1)

        # --- ã“ã“ã‹ã‚‰ãƒªãƒˆãƒ©ã‚¤å‡¦ç†è¿½åŠ  ---
        max_retries = 10
        attempt = 0
        check_div = None

        while attempt < max_retries:
            attempt += 1
            try:
                await page.wait_for_selector(
                    'div.inner.nc-text-align-right',
                    timeout=15000
                )
            except Exception as e:
                print(f"[WARN] divã‚¯ãƒªãƒƒã‚¯å¾Œã« check_div ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (è©¦è¡Œ {attempt}/{max_retries}): {e}")
                await page.reload(timeout=60000)
                # await page.go_back(timeout=60000)
                # await page.goto(full_url, timeout=60000)
                await asyncio.sleep(1)
                # å†åº¦ target_td ã‚’å–å¾—
                # target_td = await page.query_selector('td.nc-grid-color-fix.nc-text-align-center')
                # if not target_td:
                #     raise Exception("Target <td> not found after navigation.")
                # divs = await target_td.query_selector_all('div.outer.border-bottom')
                # div = divs[i]
                # await div.click(timeout=60000)
                # await asyncio.sleep(1)
                continue
            check_div = await page.query_selector('div.inner.nc-text-align-right')
            break

        if not check_div:
            raise Exception(f"âŒ æœ€å¤§ {max_retries} å›è©¦ã—ã¦ã‚‚ check_div ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

        match scrap_days:
            case 1:
                target_titles = ["1æ—¥å‰"]
            case 2:
                target_titles = ["1æ—¥å‰", "2æ—¥å‰"]
            case 3:
                target_titles = ["1æ—¥å‰", "2æ—¥å‰", "3æ—¥å‰"]
            case 4:
                target_titles = ["1æ—¥å‰", "2æ—¥å‰", "3æ—¥å‰", "4æ—¥å‰"]
            case 5:
                target_titles = ["1æ—¥å‰", "2æ—¥å‰", "3æ—¥å‰", "4æ—¥å‰", "5æ—¥å‰"]
            case 6:
                target_titles = ["1æ—¥å‰", "2æ—¥å‰", "3æ—¥å‰", "4æ—¥å‰", "5æ—¥å‰", "6æ—¥å‰"]

        # å„æ—¥ä»˜ã®å³ç«¯ãƒ‡ãƒ¼ã‚¿ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ
        right_endpoints = []
        if response_data:
            for title in target_titles:
                try:
                    graph = next(g for g in response_data["Graph"] if g["title"] == title)
                    datas = graph["src"]["datas"]
                    points = [p for p in datas if "out" in p and "value" in p]
                    right = next((p for p in reversed(points) if p["value"] != 0), points[-1])
                    right_endpoints.append({
                        "title": title,
                        "out": right["out"],
                        "value": right["value"]
                    })
                except StopIteration:
                    print(f"âš  ã‚°ãƒ©ãƒ•ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {title}")
                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼ˆ{title}ï¼‰: {e}")

            # çµæœè¡¨ç¤º
            # for data in right_endpoints:
                # print(f"ğŸ“Š {data['title']} ã®å³ç«¯: out={data['out']}, value={data['value']}")
        else:
            print("â— ã‚¹ãƒ©ãƒ³ãƒ—ã‚°ãƒ©ãƒ•ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")

        result = await eachMachineFunc(page, model_name, scrap_days)
        # print(f"å–å¾—ã—ãŸæ©Ÿç¨®ãƒ‡ãƒ¼ã‚¿: {result}")
        if result:
            for index, data in enumerate(right_endpoints):
                # å„æ—¥ä»˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
                if index < len(result):
                    result[index][3] = data['out']
                    result[index][4] = data['value']
                    append_row_to_csv(result[index], filename)
        else:
            print("â— æ©Ÿç¨®ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        for res in result:
            print(f"ğŸ“‹ å–å¾—ãƒ‡ãƒ¼ã‚¿: {res}")
        # Go back to the previous page
        await page.goto(full_url, timeout=60000)
        await page.wait_for_load_state("load")
        await asyncio.sleep(1)

        attempt = 0
        target_td = None
        while attempt < max_retries:
            attempt += 1
            print(f"[INFO] Accessing {full_url}, attempt {attempt}/{max_retries}")
        
            try:
                await page.wait_for_selector(
                    "td.nc-grid-color-fix.nc-text-align-center", 
                    timeout=30000
                )
            except Exception:
                print("[WARN] Second target element not found, retrying...")
                # await page.reload(timeout=60000)
                # await page.go_back(timeout=60000)
                # await asyncio.sleep(1)
                await page.goto(full_url, timeout=60000)
                await asyncio.sleep(1)
                continue  # æ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ãƒªãƒˆãƒ©ã‚¤

                # è¦ç´ å–å¾—
            target_td = await page.query_selector('td.nc-grid-color-fix.nc-text-align-center')
            break

        if not target_td:
            raise Exception(f"Second target <td> not found after {max_retries} retries.")

async def scrap_slot(context, shop_rows):
    page = await context.new_page()

    await page.add_init_script("""
    Object.defineProperty(navigator, 'webdriver', {
        get: () => undefined
    });
    """)
    for shop in shop_rows:
        shop_url = shop.get("åº—èˆ—URL")
        print(f"å‡¦ç†ä¸­ã®åº—èˆ—: {shop_url}")
        slot_sheet_url = shop.get("ã‚¹ãƒ­ãƒƒãƒˆç”¨")
        print(f"ã‚¹ãƒ­ãƒƒãƒˆã‚·ãƒ¼ãƒˆURL: {slot_sheet_url}")
        if not shop_url or not slot_sheet_url:
            print("åº—èˆ—URLã¾ãŸã¯ã‚¹ãƒ­ãƒƒãƒˆã‚·ãƒ¼ãƒˆURLãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        current_sheet_date_str = get_current_sheet_date(slot_sheet_url)

        if current_sheet_date_str:
            # Parse the string into a datetime object
            current_sheet_date = datetime.strptime(current_sheet_date_str, "%Y/%m/%d")
            
            # Calculate the difference (delta)
            scrap_delta = datetime.now() - current_sheet_date
            
            # Optionally, get number of days as integer
            scrap_days = min(scrap_delta.days - 1, 6)
            print(f"ç¾åœ¨ã®ã‚·ãƒ¼ãƒˆã®æ—¥ä»˜: {current_sheet_date}, å·®åˆ†æ—¥æ•°: {scrap_days} æ—¥")
        else:
            scrap_days = 6
            print(f"å·®åˆ†æ—¥æ•°: {scrap_days} æ—¥")

        if scrap_days == 0:
            print("ã‚·ãƒ¼ãƒˆã®æ—¥ä»˜ã¨ç¾åœ¨ã®æ—¥ä»˜ãŒåŒã˜ã§ã™ã€‚å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        filename = f"result(slot)-{shop_url}.csv"
        filename = sanitize_filename(filename)
        headers = [
            "æ—¥ä»˜", "æ©Ÿç¨®å", "å°ç•ªå·", "æŠ•å…¥æšæ•°", "å·®æšæ•°", 
            "BIGå›æ•°", "REGå›æ•°", "AT/ARTå›æ•°", "ç´¯è¨ˆã‚¹ã‚¿ãƒ¼ãƒˆ", "æœ€çµ‚ã‚¹ã‚¿ãƒ¼ãƒˆ"
        ]

        # Open the file in write mode (this will truncate it if it exists)
        with open(filename, mode='w', newline='', encoding='utf-8') as file:
            writer = csv.writer(file)
            writer.writerow(headers)  # Always write the header

        print(f'ãƒ•ã‚¡ã‚¤ãƒ«ã€Œ{filename}ã€ã¯ã€ãƒ˜ãƒƒãƒ€ãƒ¼ã®ã¿ã§åˆæœŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚')

        initial_page = shop_url
        # Step 1: Go to the initial page
        await page.goto(initial_page)
        await page.evaluate("window.scrollBy(0, 500)")
        await asyncio.sleep(1)

        # æ¡ä»¶ã«åˆã† img è¦ç´ ã‚’ã™ã¹ã¦å–å¾—
        img_elements = await page.query_selector_all('td a img[alt="ã‚¹ãƒ­ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿"]')

        if img_elements:
            # æœ€åˆã® img è¦ç´ ã‚’é¸æŠ
            img = img_elements[0]

            # æœ€ã‚‚è¿‘ã„è¦ª a ã‚¿ã‚°ã‚’å–å¾—
            a_element = await img.evaluate_handle("el => el.closest('a')")

            # href å±æ€§ã‚’å–å¾—
            href = await a_element.get_attribute("href")
            base_url = initial_page.rsplit("/", 1)[0] + "/" + href if href else initial_page
            print(f"ãƒ™ãƒ¼ã‚¹URL: {base_url}")
            print(f"å–å¾—ã—ãŸ href: {href}")
        else:
            print("è©²å½“è¦ç´ ãªã—")
            continue

        await page.goto(base_url, timeout=0)
        await page.wait_for_selector("ul#ulKI > li", timeout=0)

        # ä¾‹: è¨±å¯ã•ã‚ŒãŸæ©Ÿç¨®åãƒªã‚¹ãƒˆï¼ˆéƒ¨åˆ†ä¸€è‡´ãªã©ã§ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ï¼‰
        allowed_titles = ["eçœŸåŒ—æ–—ç„¡åŒ5 SFEE"]

        last_len = 0
        scroll_offset = 800
        visited_links = set()

        # continue

        while True:
            link_title_list = []
            # Scroll down incrementally
            # await page.evaluate(f"window.scrollTo(0, {scroll_offset})")
            await human_like_scroll(page, scroll_offset)
            await page.wait_for_load_state("load")

            # Get all visible <li> elements under #ulKI
            list_items = await page.query_selector_all("ul#ulKI > li")
            print(f"last_len-len(list_items):{last_len}-{len(list_items)}")

            if last_len == len(list_items): break

            for i in range(last_len, len(list_items)):
                li = list_items[i]

                link = await li.query_selector("a")
                if not link:
                    continue

                # æœ€åˆã® <div> ã‚’å–å¾—ã—ã¦ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
                divs_in_link = await link.query_selector_all("div")
                if not divs_in_link:
                    continue

                first_div = divs_in_link[0]
                title_text = (await first_div.inner_text()).strip()

                href = await link.get_attribute("href")
                if href and href not in visited_links:
                    visited_links.add(href)
                    link_title_list.append((href, title_text))
                    # print(f"âœ… ãƒªãƒ³ã‚¯åé›†: {href} / ã‚¿ã‚¤ãƒˆãƒ«: {title_text}")

            print(f"\nğŸ” åé›†ã—ãŸãƒªãƒ³ã‚¯æ•°: {len(link_title_list)}\n")


            # ğŸ” æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯ãƒ»ã‚¿ã‚¤ãƒˆãƒ«ã‚’ä½¿ã£ã¦ãƒšãƒ¼ã‚¸é·ç§»å‡¦ç†
            for href, title_text in link_title_list:
                full_url = base_url.rsplit("/", 1)[0] + "/" + href  # href ãŒç›¸å¯¾ãƒ‘ã‚¹ãªã‚‰è£œå®Œ
                print(f"\nâ¡ï¸ é·ç§»: {title_text} - {full_url}")

                # await page.goto(full_url)
                # await page.wait_for_load_state("load")

                # å®Ÿéš›ã®å‡¦ç†ã‚’å®Ÿè¡Œ
                await eachModelFunc(page, full_url, title_text, filename, scrap_days)

                # Scroll further for next round
                scroll_offset += 600

            last_len = len(list_items)
            # Go back to the list page
            await page.goto(base_url, timeout=0)
            await page.wait_for_load_state("load")

    await page.close()
    print("å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

UAS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
]
def rand_ua(): return random.choice(UAS)

async def run():
    init_csvs()
    shop_rows = get_checked_rows()
    if not shop_rows:
        print("ãƒã‚§ãƒƒã‚¯ã•ã‚ŒãŸè¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
        return

    PROXY_SERVER = "http://p.webshare.io:80"
    PROXY_USERNAME = "gzujsifm-JP-rotate"
    PROXY_PASSWORD = "2cwmhvha2ian"

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=False,
            proxy={
                "server": PROXY_SERVER,
                "username": PROXY_USERNAME,
                "password": PROXY_PASSWORD,
            }
        )
        context = await browser.new_context(
            user_agent=rand_ua(),
            viewport={"width": 1360, "height": 860},
            ignore_https_errors=True,
        )

        try:
            # ip_logger_task ã‚’ä¸¦è¡Œã§èµ°ã‚‰ã›ã‚‹ãŒã€Taskã¨ã—ã¦ä¿æŒ
            ip_task = asyncio.create_task(ip_logger_task(context))

            # ãƒ¡ã‚¤ãƒ³å‡¦ç†ãŒçµ‚ã‚ã‚‹ã¾ã§å¾…æ©Ÿ
            await scrap_slot(context, shop_rows)

        finally:
            # scrap_pachinko ãŒçµ‚ã‚ã£ãŸã‚‰ ip_logger_task ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
            ip_task.cancel()
            try:
                await ip_task
            except asyncio.CancelledError:
                print("âœ… ip_logger_task ã‚’çµ‚äº†ã—ã¾ã—ãŸã€‚")

            await context.close()
            await browser.close()

        if os.path.exists(IPLOG_CSV):
            csv_to_xlsx(IPLOG_CSV, IPLOG_XLSX)

def main(to_file=True):
    set_stdout(to_file)
    asyncio.run(run())

if __name__ == "__main__":
    main(to_file=True)
